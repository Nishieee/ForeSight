{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a8db6e6",
   "metadata": {},
   "source": [
    "# Essential Data Sanity Checks\n",
    "\n",
    "**Purpose**: Verify data integrity before feature engineering. Only the 3 critical checks needed.\n",
    "\n",
    "1. **Data Integrity**: No missing IDs, consistent row counts\n",
    "2. **Temporal Alignment**: Transaction dates within label timeline  \n",
    "3. **Label Quality**: Distress rate and signal strength\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5e2ee22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 2,500 customers, 1,122,312 transactions\n",
      "Regenerated daily_labels: 465,000 rows covering 2025-02-16 → 2025-08-20\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path(\"../data\")  # change if needed\n",
    "\n",
    "customers = pd.read_csv(DATA_DIR / \"customers.csv\", parse_dates=[\"signup_date\"])\n",
    "transactions = pd.read_csv(DATA_DIR / \"transactions.csv\", parse_dates=[\"date\"])\n",
    "outcomes = pd.read_csv(DATA_DIR / \"outcomes.csv\", parse_dates=[\"distress_start_date\", \"event_date\"])\n",
    "daily_labels = pd.read_csv(DATA_DIR / \"daily_labels.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "print(f\"Loaded: {len(customers):,} customers, {len(transactions):,} transactions\")\n",
    "\n",
    "# Ensure daily_labels covers the full transaction window\n",
    "tx_min, tx_max = transactions[\"date\"].min(), transactions[\"date\"].max()\n",
    "full_dates = pd.date_range(tx_min, tx_max, freq=\"D\")\n",
    "\n",
    "grid = (\n",
    "    customers[[\"customer_id\"]]\n",
    "    .assign(key=1)\n",
    "    .merge(pd.DataFrame({\"date\": full_dates, \"key\":1}), on=\"key\")\n",
    "    .drop(columns=\"key\")\n",
    ")\n",
    "\n",
    "# Join outcomes and recompute labels (7 days before event = 1)\n",
    "grid = grid.merge(outcomes[[\"customer_id\",\"event_date\"]], on=\"customer_id\", how=\"left\")\n",
    "\n",
    "win = 7\n",
    "grid[\"label\"] = (\n",
    "    (grid[\"event_date\"].notna()) &\n",
    "    (grid[\"date\"] >= grid[\"event_date\"] - pd.Timedelta(days=win)) &\n",
    "    (grid[\"date\"] <= grid[\"event_date\"])\n",
    ").astype(int)\n",
    "\n",
    "daily_labels = grid[[\"customer_id\",\"date\",\"label\"]]\n",
    "\n",
    "print(f\"Regenerated daily_labels: {len(daily_labels):,} rows covering {tx_min.date()} → {tx_max.date()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce847f1",
   "metadata": {},
   "source": [
    "## 1. Data Integrity Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13dc486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DATA INTEGRITY\n",
      "- Missing customer_id values: 0\n",
      "- Duplicate customer_id rows in customers: 0\n",
      "- Transactions reference valid customers: True\n",
      "✅ Data Integrity: PASS\n"
     ]
    }
   ],
   "source": [
    "# Check for missing customer IDs\n",
    "missing_ids = customers[\"customer_id\"].isna().sum() + transactions[\"customer_id\"].isna().sum()\n",
    "dupe_customers = customers[\"customer_id\"].duplicated().sum()\n",
    "\n",
    "# Foreign-key integrity: every tx customer_id exists in customers\n",
    "valid_fk = transactions[\"customer_id\"].isin(customers[\"customer_id\"]).all()\n",
    "\n",
    "integrity_pass = (missing_ids == 0) and (dupe_customers == 0) and valid_fk\n",
    "\n",
    "print(\"🔍 DATA INTEGRITY\")\n",
    "print(f\"- Missing customer_id values: {missing_ids}\")\n",
    "print(f\"- Duplicate customer_id rows in customers: {dupe_customers}\")\n",
    "print(f\"- Transactions reference valid customers: {valid_fk}\")\n",
    "print(f\"✅ Data Integrity: {'PASS' if integrity_pass else 'FAIL'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76c5f8",
   "metadata": {},
   "source": [
    "## 2. Temporal Alignment Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b2acc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🕒 TEMPORAL ALIGNMENT\n",
      "- Transactions range: 2025-02-16 → 2025-08-20\n",
      "- Labels range:       2025-02-16 → 2025-08-20\n",
      "- Tx within labels:   True\n",
      "- Events covered:     True\n",
      "✅ Temporal Alignment: PASS\n"
     ]
    }
   ],
   "source": [
    "# Convert is already parsed; compute ranges\n",
    "tx_min, tx_max = transactions[\"date\"].min(), transactions[\"date\"].max()\n",
    "lbl_min, lbl_max = daily_labels[\"date\"].min(), daily_labels[\"date\"].max()\n",
    "\n",
    "within_timeline = (tx_min >= lbl_min) and (tx_max <= lbl_max)\n",
    "\n",
    "# Outcomes present should fall inside the label span\n",
    "events = outcomes[\"event_date\"].dropna()\n",
    "events_covered = events.between(lbl_min, lbl_max).all() if len(events) else True\n",
    "\n",
    "temporal_pass = within_timeline and events_covered\n",
    "\n",
    "print(\"\\n🕒 TEMPORAL ALIGNMENT\")\n",
    "print(f\"- Transactions range: {tx_min.date()} → {tx_max.date()}\")\n",
    "print(f\"- Labels range:       {lbl_min.date()} → {lbl_max.date()}\")\n",
    "print(f\"- Tx within labels:   {within_timeline}\")\n",
    "print(f\"- Events covered:     {events_covered}\")\n",
    "print(f\"✅ Temporal Alignment: {'PASS' if temporal_pass else 'FAIL'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a2f0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX: 2025-02-16 → 2025-08-20\n",
      "LB: 2025-02-16 → 2025-08-20\n",
      "EV: 2025-05-08 → 2025-08-05\n",
      "\n",
      "Which condition failed?\n",
      "- tx_min < lbl_min: False\n",
      "- tx_max > lbl_max: False\n",
      "- any event < lbl_min: False\n",
      "- any event > lbl_max: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1t/zpc_3c3d039cn7bcqzxdmsz80000gn/T/ipykernel_57354/1894370959.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_labels[\"date\"] = pd.to_datetime(daily_labels[\"date\"])\n"
     ]
    }
   ],
   "source": [
    "# make 100% sure dates are datetime\n",
    "transactions[\"date\"] = pd.to_datetime(transactions[\"date\"])\n",
    "daily_labels[\"date\"] = pd.to_datetime(daily_labels[\"date\"])\n",
    "outcomes[\"event_date\"] = pd.to_datetime(outcomes[\"event_date\"])\n",
    "\n",
    "tx_min, tx_max = transactions[\"date\"].min(), transactions[\"date\"].max()\n",
    "lbl_min, lbl_max = daily_labels[\"date\"].min(), daily_labels[\"date\"].max()\n",
    "ev_min, ev_max = outcomes[\"event_date\"].min(), outcomes[\"event_date\"].max()\n",
    "\n",
    "print(\"TX:\", tx_min.date(), \"→\", tx_max.date())\n",
    "print(\"LB:\", lbl_min.date(), \"→\", lbl_max.date())\n",
    "print(\"EV:\", ev_min.date() if pd.notna(ev_min) else None, \"→\", ev_max.date() if pd.notna(ev_max) else None)\n",
    "\n",
    "print(\"\\nWhich condition failed?\")\n",
    "print(\"- tx_min < lbl_min:\", tx_min < lbl_min)\n",
    "print(\"- tx_max > lbl_max:\", tx_max > lbl_max)\n",
    "print(\"- any event < lbl_min:\", outcomes[\"event_date\"].dropna().lt(lbl_min).any())\n",
    "print(\"- any event > lbl_max:\", outcomes[\"event_date\"].dropna().gt(lbl_max).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b5cd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏷️  LABEL QUALITY\n",
      "- Distress rate: 0.192\n",
      "- High-risk spend pre-event vs. baseline (x): 1.53\n",
      "✅ Label Quality: PASS\n"
     ]
    }
   ],
   "source": [
    "# Distress rate from outcomes\n",
    "distress_rate = outcomes[\"is_distressed\"].mean()\n",
    "\n",
    "# Simple signal probe: avg high-risk spend 7 days before event vs. far-away period\n",
    "tx_hr = transactions[transactions[\"category\"].isin([\"cash_advance\",\"payday_loan\"])]\n",
    "\n",
    "# Join tx to outcomes to know customer event date\n",
    "ev = outcomes[[\"customer_id\",\"event_date\"]].dropna()\n",
    "tx_ev = tx_hr.merge(ev, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "pre_mask = (tx_ev[\"date\"] >= (tx_ev[\"event_date\"] - pd.Timedelta(days=7))) & (tx_ev[\"date\"] <= tx_ev[\"event_date\"])\n",
    "far_mask = (tx_ev[\"date\"] <= (tx_ev[\"event_date\"] - pd.Timedelta(days=30))) & (tx_ev[\"date\"] >= (tx_ev[\"event_date\"] - pd.Timedelta(days=60)))\n",
    "\n",
    "pre_avg = tx_ev.loc[pre_mask, \"amount\"].abs().mean()\n",
    "far_avg = tx_ev.loc[far_mask, \"amount\"].abs().mean()\n",
    "\n",
    "signal_uplift = (pre_avg / far_avg) if (far_avg and not pd.isna(far_avg)) else float(\"nan\")\n",
    "\n",
    "# Heuristics for pass/fail\n",
    "rate_ok = 0.05 <= distress_rate <= 0.30\n",
    "signal_ok = (signal_uplift >= 1.15) if pd.notna(signal_uplift) else False  # at least +15%\n",
    "\n",
    "label_pass = rate_ok and signal_ok\n",
    "\n",
    "print(\"\\n🏷️  LABEL QUALITY\")\n",
    "print(f\"- Distress rate: {distress_rate:.3f}\")\n",
    "print(f\"- High-risk spend pre-event vs. baseline (x): {signal_uplift:.2f}\" if pd.notna(signal_uplift) else \"- Not enough events to compute uplift\")\n",
    "print(f\"✅ Label Quality: {'PASS' if label_pass else 'FAIL'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a295621",
   "metadata": {},
   "source": [
    "## Final Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e7942fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 SANITY CHECK RESULTS: 3/3 PASSED\n",
      "========================================\n",
      "🎉 EXCELLENT: Data ready for feature engineering!\n",
      "\n",
      "Dataset: 2,500 customers, 1,122,312 transactions\n",
      "Ready for next step: Feature engineering\n"
     ]
    }
   ],
   "source": [
    "all_checks = [integrity_pass, temporal_pass, label_pass]\n",
    "score = sum(all_checks)\n",
    "\n",
    "print(f\"\\n🎯 SANITY CHECK RESULTS: {score}/3 PASSED\")\n",
    "print(\"=\" * 40)\n",
    "if score == 3:\n",
    "    print(\"🎉 EXCELLENT: Data ready for feature engineering!\")\n",
    "elif score == 2:\n",
    "    print(\"✅ GOOD: Minor issues, proceed with caution\")\n",
    "else:\n",
    "    print(\"❌ FAIL: Fix data issues before modeling\")\n",
    "\n",
    "print(f\"\\nDataset: {len(customers):,} customers, {len(transactions):,} transactions\")\n",
    "print(\"Ready for next step: Feature engineering\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
