{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering for Financial Distress Prediction\n",
        "\n",
        "**Purpose**: Transform transaction data into ML-ready features with temporal structure.\n",
        "\n",
        "## Pipeline Steps:\n",
        "1. **Daily Aggregations**: Transaction summaries per customer-day\n",
        "2. **Rolling Windows**: 30-day moving statistics (sums, averages, ratios)\n",
        "3. **Drift Features**: Recent behavior vs. baseline comparisons\n",
        "4. **Label Integration**: Join with daily_labels for supervised learning\n",
        "5. **Temporal Split**: Train/test split respecting time order\n",
        "\n",
        "**Output**: ML-ready dataset with features and labels, properly split for time-series validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Loaded: 2,500 customers, 1,122,312 transactions\n",
            "Date range: 2025-02-16 → 2025-08-20\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load data\n",
        "DATA_DIR = Path(\"../data\")\n",
        "print(\"Loading data...\")\n",
        "\n",
        "customers = pd.read_csv(DATA_DIR / \"customers.csv\", parse_dates=[\"signup_date\"])\n",
        "transactions = pd.read_csv(DATA_DIR / \"transactions.csv\", parse_dates=[\"date\"])\n",
        "outcomes = pd.read_csv(DATA_DIR / \"outcomes.csv\", parse_dates=[\"distress_start_date\", \"event_date\"])\n",
        "daily_labels = pd.read_csv(DATA_DIR / \"daily_labels.csv\", parse_dates=[\"date\"])\n",
        "\n",
        "print(f\"Loaded: {len(customers):,} customers, {len(transactions):,} transactions\")\n",
        "print(f\"Date range: {transactions['date'].min().date()} → {transactions['date'].max().date()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Daily Transaction Aggregations\n",
        "\n",
        "Create comprehensive daily summaries per customer with category breakdowns and behavioral indicators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aggregating daily features...\n",
            "Created daily features: 410,073 customer-days, 20 features\n",
            "\\nFeature columns: ['customer_id', 'date', 'total_flow', 'tx_count', 'unique_merchants', 'total_income', 'total_spend', 'spend_cash_advance', 'spend_dining', 'spend_ecommerce', 'spend_entertainment', 'spend_groceries', 'spend_payday_loan', 'spend_rent', 'spend_transport', 'spend_utilities', 'cash_advance_amt', 'payday_loan_amt', 'net_flow', 'is_weekend']\n"
          ]
        }
      ],
      "source": [
        "def create_daily_features(transactions_df):\n",
        "    \"\"\"\n",
        "    Aggregate transactions into daily features per customer.\n",
        "    \n",
        "    Returns DataFrame with columns:\n",
        "    - Basic: total_spend, total_income, net_flow, tx_count\n",
        "    - Categories: spend by category (groceries, transport, etc.)\n",
        "    - Risk indicators: cash_advance_amt, payday_loan_amt, bills_delayed\n",
        "    - Behavioral: unique_merchants, weekend_spend_ratio\n",
        "    \"\"\"\n",
        "    \n",
        "    # Separate income vs spending\n",
        "    income_tx = transactions_df[transactions_df['is_income'] == True].copy()\n",
        "    spend_tx = transactions_df[transactions_df['is_income'] == False].copy()\n",
        "    \n",
        "    print(\"Aggregating daily features...\")\n",
        "    \n",
        "    # Basic daily aggregations\n",
        "    daily_basic = transactions_df.groupby(['customer_id', 'date']).agg({\n",
        "        'amount': ['sum', 'count'],\n",
        "        'merchant': 'nunique'\n",
        "    }).round(2)\n",
        "    \n",
        "    daily_basic.columns = ['total_flow', 'tx_count', 'unique_merchants']\n",
        "    daily_basic = daily_basic.reset_index()\n",
        "    \n",
        "    # Income aggregations\n",
        "    daily_income = income_tx.groupby(['customer_id', 'date'])['amount'].sum().reset_index()\n",
        "    daily_income.columns = ['customer_id', 'date', 'total_income']\n",
        "    \n",
        "    # Spending aggregations (negative amounts, so we'll make them positive)\n",
        "    spend_tx['spend_amount'] = -spend_tx['amount']  # Convert to positive spending\n",
        "    \n",
        "    daily_spend = spend_tx.groupby(['customer_id', 'date'])['spend_amount'].sum().reset_index()\n",
        "    daily_spend.columns = ['customer_id', 'date', 'total_spend']\n",
        "    \n",
        "    # Category breakdown (spending only)\n",
        "    category_spend = spend_tx.groupby(['customer_id', 'date', 'category'])['spend_amount'].sum().unstack(fill_value=0)\n",
        "    category_spend = category_spend.reset_index()\n",
        "    \n",
        "    # Add category prefixes to avoid confusion\n",
        "    category_cols = [col for col in category_spend.columns if col not in ['customer_id', 'date']]\n",
        "    category_spend = category_spend.rename(columns={col: f'spend_{col}' for col in category_cols})\n",
        "    \n",
        "    # Risk indicators\n",
        "    risk_categories = ['cash_advance', 'payday_loan']\n",
        "    risk_tx = spend_tx[spend_tx['category'].isin(risk_categories)]\n",
        "    \n",
        "    daily_risk = risk_tx.groupby(['customer_id', 'date', 'category'])['spend_amount'].sum().unstack(fill_value=0)\n",
        "    daily_risk = daily_risk.reset_index()\n",
        "    if 'cash_advance' in daily_risk.columns:\n",
        "        daily_risk = daily_risk.rename(columns={'cash_advance': 'cash_advance_amt'})\n",
        "    if 'payday_loan' in daily_risk.columns:\n",
        "        daily_risk = daily_risk.rename(columns={'payday_loan': 'payday_loan_amt'})\n",
        "    \n",
        "    # Weekend indicator\n",
        "    transactions_df['is_weekend'] = transactions_df['date'].dt.dayofweek >= 5\n",
        "    weekend_spend = spend_tx.merge(transactions_df[['customer_id', 'date', 'is_weekend']], \n",
        "                                   on=['customer_id', 'date'], how='left')\n",
        "    \n",
        "    daily_weekend = weekend_spend.groupby(['customer_id', 'date']).agg({\n",
        "        'spend_amount': 'sum',\n",
        "        'is_weekend': 'first'\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Merge all daily features\n",
        "    daily_features = daily_basic\n",
        "    \n",
        "    for df in [daily_income, daily_spend, category_spend, daily_risk]:\n",
        "        daily_features = daily_features.merge(df, on=['customer_id', 'date'], how='left')\n",
        "    \n",
        "    # Fill missing values\n",
        "    daily_features = daily_features.fillna(0)\n",
        "    \n",
        "    # Calculate net flow\n",
        "    daily_features['net_flow'] = daily_features['total_income'] - daily_features['total_spend']\n",
        "    \n",
        "    # Add weekend flag\n",
        "    daily_features['is_weekend'] = daily_features['date'].dt.dayofweek >= 5\n",
        "    \n",
        "    return daily_features\n",
        "\n",
        "# Create daily features\n",
        "daily_features = create_daily_features(transactions)\n",
        "print(f\"Created daily features: {len(daily_features):,} customer-days, {len(daily_features.columns)} features\")\n",
        "print(\"\\\\nFeature columns:\", list(daily_features.columns))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Rolling Window Features\n",
        "\n",
        "Build 30-day rolling statistics to capture recent behavioral patterns and trends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing 30-day rolling features...\n",
            "Added rolling features: 59 total columns\n",
            "\\nKey rolling features (38): ['total_spend_30d_sum', 'total_income_30d_sum', 'net_flow_30d_sum', 'tx_count_30d_sum', 'cash_advance_amt_30d_sum', 'payday_loan_amt_30d_sum', 'unique_merchants_30d_sum', 'spend_cash_advance_30d_sum', 'spend_dining_30d_sum', 'spend_ecommerce_30d_sum']...\n"
          ]
        }
      ],
      "source": [
        "def add_rolling_features(daily_df, window=30):\n",
        "    \"\"\"\n",
        "    Add rolling window features (30-day moving statistics).\n",
        "    \n",
        "    For each customer, compute rolling:\n",
        "    - Sums: spending, income, transaction counts\n",
        "    - Averages: daily spending patterns\n",
        "    - Ratios: cash advance / total spend, etc.\n",
        "    - Volatility: standard deviation of daily patterns\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"Computing {window}-day rolling features...\")\n",
        "    \n",
        "    # Sort by customer and date\n",
        "    df = daily_df.sort_values(['customer_id', 'date']).copy()\n",
        "    \n",
        "    # Key columns for rolling stats\n",
        "    rolling_cols = [\n",
        "        'total_spend', 'total_income', 'net_flow', 'tx_count',\n",
        "        'cash_advance_amt', 'payday_loan_amt', 'unique_merchants'\n",
        "    ]\n",
        "    \n",
        "    # Add category spending if they exist\n",
        "    category_cols = [col for col in df.columns if col.startswith('spend_') and col != 'spend_amount']\n",
        "    rolling_cols.extend(category_cols)\n",
        "    \n",
        "    # Rolling sums (30-day totals)\n",
        "    rolling_sums = df.groupby('customer_id')[rolling_cols].rolling(\n",
        "        window=window, min_periods=1\n",
        "    ).sum().reset_index(level=0, drop=True)\n",
        "    \n",
        "    rolling_sums = rolling_sums.add_suffix(f'_{window}d_sum')\n",
        "    \n",
        "    # Rolling means (30-day averages)\n",
        "    rolling_means = df.groupby('customer_id')[rolling_cols].rolling(\n",
        "        window=window, min_periods=1\n",
        "    ).mean().reset_index(level=0, drop=True)\n",
        "    \n",
        "    rolling_means = rolling_means.add_suffix(f'_{window}d_avg')\n",
        "    \n",
        "    # Rolling standard deviations (volatility)\n",
        "    volatility_cols = ['total_spend', 'net_flow', 'tx_count']\n",
        "    rolling_stds = df.groupby('customer_id')[volatility_cols].rolling(\n",
        "        window=window, min_periods=2\n",
        "    ).std().reset_index(level=0, drop=True)\n",
        "    \n",
        "    rolling_stds = rolling_stds.add_suffix(f'_{window}d_std')\n",
        "    \n",
        "    # Combine with original data\n",
        "    result = pd.concat([df, rolling_sums, rolling_means, rolling_stds], axis=1)\n",
        "    \n",
        "    # Derived ratios\n",
        "    result[f'cash_advance_ratio_{window}d'] = (\n",
        "        result[f'cash_advance_amt_{window}d_sum'] / \n",
        "        (result[f'total_spend_{window}d_sum'] + 1e-8)  # Avoid division by zero\n",
        "    )\n",
        "    \n",
        "    result[f'payday_loan_ratio_{window}d'] = (\n",
        "        result[f'payday_loan_amt_{window}d_sum'] / \n",
        "        (result[f'total_spend_{window}d_sum'] + 1e-8)\n",
        "    )\n",
        "    \n",
        "    result[f'income_spend_ratio_{window}d'] = (\n",
        "        result[f'total_income_{window}d_sum'] / \n",
        "        (result[f'total_spend_{window}d_sum'] + 1e-8)\n",
        "    )\n",
        "    \n",
        "    # Transaction frequency\n",
        "    result[f'avg_daily_tx_{window}d'] = result[f'tx_count_{window}d_sum'] / window\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Add rolling features\n",
        "features_with_rolling = add_rolling_features(daily_features, window=30)\n",
        "print(f\"Added rolling features: {len(features_with_rolling.columns)} total columns\")\n",
        "\n",
        "# Show some key rolling features\n",
        "rolling_feature_cols = [col for col in features_with_rolling.columns if '_30d_' in col or '_ratio_' in col]\n",
        "print(f\"\\\\nKey rolling features ({len(rolling_feature_cols)}): {rolling_feature_cols[:10]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Drift Features\n",
        "\n",
        "Compare recent behavior (last 7-14 days) against baseline (earlier period) to detect behavioral changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing drift features (recent 7d vs baseline 30d)...\n",
            "Added drift features: 76 total columns\n",
            "\\nDrift features (7): ['total_spend_drift_ratio', 'cash_advance_amt_drift_ratio', 'payday_loan_amt_drift_ratio', 'tx_count_drift_ratio', 'unique_merchants_drift_ratio', 'risk_spend_drift', 'spend_acceleration']\n"
          ]
        }
      ],
      "source": [
        "def add_drift_features(df, recent_days=7, baseline_days=30):\n",
        "    \"\"\"\n",
        "    Add drift features comparing recent vs baseline behavior.\n",
        "    \n",
        "    For each customer-date:\n",
        "    - Recent: average of last 7 days\n",
        "    - Baseline: average of days 14-44 ago (avoiding overlap)\n",
        "    - Drift: recent / baseline ratios\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"Computing drift features (recent {recent_days}d vs baseline {baseline_days}d)...\")\n",
        "    \n",
        "    df = df.sort_values(['customer_id', 'date']).copy()\n",
        "    \n",
        "    # Columns to analyze for drift\n",
        "    drift_cols = [\n",
        "        'total_spend', 'cash_advance_amt', 'payday_loan_amt', \n",
        "        'tx_count', 'unique_merchants'\n",
        "    ]\n",
        "    \n",
        "    # Recent period (last 7 days)\n",
        "    recent_stats = df.groupby('customer_id')[drift_cols].rolling(\n",
        "        window=recent_days, min_periods=1\n",
        "    ).mean().reset_index(level=0, drop=True)\n",
        "    recent_stats = recent_stats.add_suffix('_recent_avg')\n",
        "    \n",
        "    # Baseline period (days 14-44 ago, shifted to avoid overlap)\n",
        "    # We'll use a longer rolling window and shift it\n",
        "    baseline_stats = df.groupby('customer_id')[drift_cols].rolling(\n",
        "        window=baseline_days, min_periods=5  # Need at least 5 days for baseline\n",
        "    ).mean().shift(14).reset_index(level=0, drop=True)  # Shift 14 days back\n",
        "    baseline_stats = baseline_stats.add_suffix('_baseline_avg')\n",
        "    \n",
        "    # Combine\n",
        "    result = pd.concat([df, recent_stats, baseline_stats], axis=1)\n",
        "    \n",
        "    # Calculate drift ratios\n",
        "    for col in drift_cols:\n",
        "        recent_col = f'{col}_recent_avg'\n",
        "        baseline_col = f'{col}_baseline_avg'\n",
        "        drift_col = f'{col}_drift_ratio'\n",
        "        \n",
        "        result[drift_col] = (\n",
        "            result[recent_col] / (result[baseline_col] + 1e-8)\n",
        "        )\n",
        "        \n",
        "        # Cap extreme ratios\n",
        "        result[drift_col] = result[drift_col].clip(0, 10)\n",
        "    \n",
        "    # Special risk drift indicators\n",
        "    result['risk_spend_drift'] = (\n",
        "        (result['cash_advance_amt_recent_avg'] + result['payday_loan_amt_recent_avg']) /\n",
        "        (result['cash_advance_amt_baseline_avg'] + result['payday_loan_amt_baseline_avg'] + 1e-8)\n",
        "    ).clip(0, 10)\n",
        "    \n",
        "    # Spending acceleration (recent vs 30-day average)\n",
        "    result['spend_acceleration'] = (\n",
        "        result['total_spend_recent_avg'] / (result['total_spend_30d_avg'] + 1e-8)\n",
        "    ).clip(0, 5)\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Add drift features\n",
        "features_with_drift = add_drift_features(features_with_rolling)\n",
        "print(f\"Added drift features: {len(features_with_drift.columns)} total columns\")\n",
        "\n",
        "# Show drift feature columns\n",
        "drift_feature_cols = [col for col in features_with_drift.columns if 'drift' in col or 'acceleration' in col]\n",
        "print(f\"\\\\nDrift features ({len(drift_feature_cols)}): {drift_feature_cols}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Add Customer Static Features\n",
        "\n",
        "Merge customer demographics and account information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added customer features: 85 total columns\n",
            "Final feature dataset: 410,073 rows × 85 columns\n"
          ]
        }
      ],
      "source": [
        "# Add customer static features\n",
        "customer_features = customers[[\n",
        "    'customer_id', 'age', 'tenure_months', 'base_income', \n",
        "    'credit_limit', 'rent_amount', 'util_weekly'\n",
        "]].copy()\n",
        "\n",
        "# Derived customer features\n",
        "customer_features['rent_income_ratio'] = -customer_features['rent_amount'] / customer_features['base_income']\n",
        "customer_features['credit_utilization_capacity'] = customer_features['credit_limit'] / customer_features['base_income']\n",
        "customer_features['monthly_util_cost'] = -customer_features['util_weekly'] * 4.3\n",
        "\n",
        "# Merge with daily features\n",
        "all_features = features_with_drift.merge(customer_features, on='customer_id', how='left')\n",
        "\n",
        "print(f\"Added customer features: {len(all_features.columns)} total columns\")\n",
        "print(f\"Final feature dataset: {len(all_features):,} rows × {len(all_features.columns)} columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Join with Labels\n",
        "\n",
        "Add the target variable for supervised learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ML dataset with labels: 408,749 rows\n",
            "Label distribution: {0: 404939, 1: 3810}\n",
            "Positive label rate: 0.009\n",
            "\\nTotal features for modeling: 82\n"
          ]
        }
      ],
      "source": [
        "# Join with daily labels\n",
        "ml_dataset = all_features.merge(\n",
        "    daily_labels[['customer_id', 'date', 'label']], \n",
        "    on=['customer_id', 'date'], \n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "print(f\"ML dataset with labels: {len(ml_dataset):,} rows\")\n",
        "print(f\"Label distribution: {ml_dataset['label'].value_counts().to_dict()}\")\n",
        "print(f\"Positive label rate: {ml_dataset['label'].mean():.3f}\")\n",
        "\n",
        "# Show feature summary\n",
        "feature_cols = [col for col in ml_dataset.columns if col not in \n",
        "               ['customer_id', 'date', 'label', 'is_weekend']]\n",
        "print(f\"\\\\nTotal features for modeling: {len(feature_cols)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Temporal Train/Test Split\n",
        "\n",
        "Split data chronologically to respect time-series nature and avoid data leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temporal split:\n",
            "  Data range: 2025-02-16 → 2025-08-14\n",
            "  Train: 2025-02-16 → 2025-07-08\n",
            "  Gap: 2025-07-08 → 2025-07-15 (7 days)\n",
            "  Test: 2025-07-15 → 2025-08-14 (30 days)\n",
            "\\nSplit results:\n",
            "  Train: 324,642 rows, 0.009 positive rate\n",
            "  Test:  70,633 rows, 0.010 positive rate\n",
            "\\nCustomer coverage:\n",
            "  Train: 2,500 unique customers\n",
            "  Test:  2,500 unique customers\n",
            "  Overlap: 2,500 customers in both sets\n"
          ]
        }
      ],
      "source": [
        "def temporal_train_test_split(df, test_days=30, gap_days=7):\n",
        "    \"\"\"\n",
        "    Split dataset temporally with a gap to prevent leakage.\n",
        "    \n",
        "    Args:\n",
        "        df: Dataset with 'date' column\n",
        "        test_days: Number of days for test set (from the end)\n",
        "        gap_days: Gap between train and test to prevent leakage\n",
        "    \n",
        "    Returns:\n",
        "        train_df, test_df\n",
        "    \"\"\"\n",
        "    \n",
        "    max_date = df['date'].max()\n",
        "    min_date = df['date'].min()\n",
        "    \n",
        "    # Define split points\n",
        "    test_start = max_date - pd.Timedelta(days=test_days)\n",
        "    train_end = test_start - pd.Timedelta(days=gap_days)\n",
        "    \n",
        "    print(f\"Temporal split:\")\n",
        "    print(f\"  Data range: {min_date.date()} → {max_date.date()}\")\n",
        "    print(f\"  Train: {min_date.date()} → {train_end.date()}\")\n",
        "    print(f\"  Gap: {train_end.date()} → {test_start.date()} ({gap_days} days)\")\n",
        "    print(f\"  Test: {test_start.date()} → {max_date.date()} ({test_days} days)\")\n",
        "    \n",
        "    # Split\n",
        "    train_df = df[df['date'] <= train_end].copy()\n",
        "    test_df = df[df['date'] >= test_start].copy()\n",
        "    \n",
        "    return train_df, test_df\n",
        "\n",
        "# Perform temporal split\n",
        "train_data, test_data = temporal_train_test_split(ml_dataset, test_days=30, gap_days=7)\n",
        "\n",
        "print(f\"\\\\nSplit results:\")\n",
        "print(f\"  Train: {len(train_data):,} rows, {train_data['label'].mean():.3f} positive rate\")\n",
        "print(f\"  Test:  {len(test_data):,} rows, {test_data['label'].mean():.3f} positive rate\")\n",
        "\n",
        "# Customer coverage\n",
        "train_customers = train_data['customer_id'].nunique()\n",
        "test_customers = test_data['customer_id'].nunique()\n",
        "overlap_customers = len(set(train_data['customer_id']) & set(test_data['customer_id']))\n",
        "\n",
        "print(f\"\\\\nCustomer coverage:\")\n",
        "print(f\"  Train: {train_customers:,} unique customers\")\n",
        "print(f\"  Test:  {test_customers:,} unique customers\")\n",
        "print(f\"  Overlap: {overlap_customers:,} customers in both sets\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "my_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
